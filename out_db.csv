title,abstract,authors,keywords,doi,url,year,asreview_label,asreview_time,asreview_note
Natural language addressing,"Large unstructured or semi-structured datasets require a high level of computational sophistication because operations that are easy at a small scale — such as moving data betweenmachines or in and out of storage, visualizing the data, or displaying results —can all require substantial algorithmic ingenuity. As a data set becomes increasingly massive, it may be infeasible to gather it in one place and analyze it as a whole. Thus, there may be a need for algorithms that operate in a distributed fashion, analyzing subsets of the data and aggregating those results to understand the complete set. One aspect of this is the challenge of data assimilation, in which we wish to use new data to update model parameters without reanalyzing the entire data set. This is essential when new waves of data continue to arrive, or subsets are analyzed in isolation of one another, and one aims to improve the model and inferences in an adaptive fashion — for example, with streaming algorithms [NRC, 2013].",Markov Krassimir; Ivanova Krassimira; Vanhoof Koen; Velychko Vitalii,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2015,0,2025-09-05 15:17,
RDFArM-a system for storing large sets of RDF triples and quadruples by means of natural language addressing,In this paper we present results from experiments for storing middle-size and large sets of RDF triples and quadruples by means of Natural Language Addressing. For experiments we have realized program RDFArM aimed to store RDF triples and quadruples in multi-layer hash tables (information spaces with variable size). The main features of program RDFArM are outlined in the paper. Analysis of the experimental results and rank-based multiple comparison are discussed.,Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2012,0,2025-09-05 15:18,
PDF,,,,,file:///Users/arjen/Zotero/storage/2S8HC6TG/Markov - 2004 - Multi-domain information model.pdf,,,,
ALGORITHM FOR QUICK NUMBERING OF LARGE VOLUMES OF DATA,"An original algorithm for numbering large datasets by means of Natural Language Addressing (NLA) is presented in the paper. We use a counter to number different instances and store its current value in the container NL-addressed by the instance. If the instance is repeated, from this NL-address we receive its already assigned number. The algorithm is implemented in an experimental program RDFArM for storing large RDF-datasets. The provided experiments have shown that NL-access time for one instance (triple or quadruple) does not depend on number of already stored instances from the dataset. This is very important for storing Big Data.",Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2015,0,2025-09-05 15:18,
RDFArM-a system for storing large sets of RDF triples and quadruples by means of natural language addressing,In this paper we present results from experiments for storing middle-size and large sets of RDF triples and quadruples by means of Natural Language Addressing. For experiments we have realized program RDFArM aimed to store RDF triples and quadruples in multi-layer hash tables (information spaces with variable size). The main features of program RDFArM are outlined in the paper. Analysis of the experimental results and rank-based multiple comparison are discussed.,Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2012,0,2025-09-05 15:18,
Distributed virtual laboratories for smart sensor system design,"In the article it is considered preconditions and main principles of creation of virtual laboratories for computer-aided design, as tools for interdisciplinary researches. An important feature of this project is using the advanced multi-dimensional access method for organizing the information base of the Virtual laboratory. Virtual laboratory, what are offered, is worth to be used on the stage of the requirements specification or EFT-stage, because it gives the possibility of fast estimating of the project realization, certain characteristics and, as a result, expected benefit of its applications. Using of these technologies already increase automation level of design stages of new devices for different purposes. Proposed computer technology gives possibility to specialists from such scientific fields, as chemistry, biology, biochemistry, physics etc, to check possibility of device creating on the basis of developed sensors. It lets to reduce terms and costs of designing of computer devices and systems on the early stages of designing, for example on the stage of requirements specification or EFT-stage. An instance of using the VLCAD is designing the Portable Device"" Floratest"" as Tool for Estimating of Megalopolis Ecology State. Portable device"" Floratest"" is aimed for express-diagnostic of plant state. It is developed in the VM Glushkov Institute of Cybernetics of National Academy of Sciences of Ukraine. Party of this device is manufactured and transferred to organizations, worked in the agricultural sector, environmental protection area, mineral fertilizer production etc. for working out of methodical tools. Using of the device for estimating of megalopolis ecology state by means of evaluation of green plant state is described in the article. Together with Megalopolis Ecomonitoring and Biodiversity Research Center of National Academy of Sciences of Ukraine there were got results of experimental researches of influence detecting of heavy metals and harmful substances on the trees and plants in Kiev.",Velychko Vitalii; Palagin Oleksandr; Romanov Volodymyr; Galelyuka Igor; Fedak Volodymyr; Grusha Volodymyr; Artemenko Dmytro; Galelyuka Oksana,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2009,0,2025-09-05 15:17,
,,,,,,,,,
Multi-domain virtual network embedding with coordinated link mapping,"Network Virtualization, which allows the coexistence of various logical networks on shared physical infrastructure, has become popular in recent years. The optimal mapping of virtual resource to physical resource is a major issue in network virtualization. This problem, called virtual network embedding (VNE), has been well explored in the context of one physical domain, which is in practice operated by a single infrastructure provider (InP). However, the needs of virtual network (VN) is rapidly growing, and quite a number of VNs have to be established across multi-domain. For multi-domain VNE, infrastructure providers can no longer just solve their own single domain VNE problem, but have to cooperate to build the whole VN. Therefore, new challenge arises for the multi-domain VNE, compared to traditional single domain VNE. The existing investigations on this problem mainly focus on decomposing a VN to sub VN for each domain, but little attention has been paid to the joint relation between intra-domain and inter-domain (peering) links. In this paper, we propose a multi-domain link mapping method which combines the intra and peering link mapping so as to optimize the overall resource utilization. Our approach is easy to be deployed since it is based on current Internet architecture. Evaluation shows that our approach brings improvements related to existing methods.",Li Shuopeng; Saidi Mohand Yazid; Chen Ken,Bandwidth; III-V semiconductor materials; Indium phosphide; Peer-to-peer computing; Substrates; Topology; Virtualization; review:Decision=excluded; review:Time=2025-09-05 15:17,10.1109/SOFTCOM.2016.7772158,,2016,0,2025-09-05 15:17,
,,,,,,,,,
,,,,,,,,,
OntoArM-a system for storing ontologies by natural language addressing,In this paper we present results from experiments for storing RDF ontologies by means of Natural Language Addressing. For experiments we have realized system OntoArM aimed to store RDF triples in multilayer hash tables (information spaces with variable size). The main features of system OntoArM are outlined in the paper. Analysis of the experimental results concluded the work.,Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2014,0,2025-09-05 15:18,
,,,,,,,,,
Storing data using natural language addressing,"Large unstructured or semi-structured datasets require a high level of computational sophistication because operations that are easy at a small scale — such as moving data between machines or in and out of storage, visualizing the data, or displaying results —can all require substantial algorithmic ingenuity. As a data set becomes increasingly massive, it may be infeasible to gather it in one place and analyze it as a whole. Thus, there may be a need for algorithms that operate in a distributed fashion, analyzing subsets of the data and aggregating those results to understand the complete set. One aspect of this is the challenge of data assimilation, in which we wish to use new data to update model parameters without reanalyzing the entire data set. This is essential when new waves of data continue to arrive, or subsets are analyzed in isolation of one another, and one aims to improve the model and inferences in an adaptive fashion — for example, with streaming algorithms [NRC, 2013]. In accordance with the actuality of these problems, firstly in [Mitov, 2011] and after that in [Markov et al, 2013] a new idea has been proposed. It is a method for effective building and storing of pattern sets in multi-layer structures during the process of associative rule mining using the possibilities of multi-dimensional numbered information spaces. The main algorithm was called “MPGN”, an abbreviation from ""Multi-layer Pyramidal Growing Networks of information spaces"". The main goal was to extend the possibilities of network structures by using a special kind of multi-layer memory structures called ""pyramids"", which permits defining and realizing new opportunities. The bottleneck of MPGN became the need to search in billions of values of the association rules’ features to convert instances in numbered arrays (vectors). This is a part of preprocessing step of algorithm (see page 97 of [Mitov, 2011]). The process of numbering took considerable time. After numbering, the MPGN algorithm had shown very good results. This work is aimed to propose a solution of the problem of searching in big index structures by proposing a special kind of hashing, so-called “multi-layer hashing”, i.e. by implementing recursively the same specialized hash function to build and resolve the collisions in hash tables. In other words, the main idea consists in using the specialized hashing functions in depth till it is needed. This approach is called “Natural Language Addressing” (NLA) [Ivanova et al, 2012a; Ivanova et al, 2013a; Ivanova et al, 2013d]. The common sense meaning of the concept “address” is such as a description of the location (of a person or organization), as written or printed on mail as directions for delivery [AHD, 2009]; the conventional form by which the location of a building is described [Collins, 2003]; a sign in front of a house or business carrying the conventional form by which its location is described; [WordNet, 2012]. We will use the concept “address” in the sense accepted in the Computer Science: the code that identifies where a piece of information is stored [WordNet, 2012]; a name or number used in information storage or retrieval that is assigned to a specific memory location; the memory location identified by this name or number [AHD, 2009]. Natural Language Addressing (NLA) is a possibility to access information using natural language words as paths to the information. For this purpose the internal encoding of the letters is used to generate corresponded path. The idea of Natural Language Addressing (NL-Addressing) is very simple. It is based on the computer internal representation of the word as strings of codes in a system of encoding (ASCII, UNICODE, etc.). For example, the ASCII encoding of the word „accession” has the next representation: (97, 99, 99, 101, 115, 115, 105, 111, 110). It may be used as array for multi-layer hashing, which indicates a path to point, where the corresponded information may be stored. The main problem in such approach is that the words have different lengths and, in addition, several words may form one phrase and this way to be assumed as single concept. This means that we need tools for managing multi-layer hashing with variable path lengths in an integrated structure. Due to the complexity of MPGN algorithm and the corresponded program system realized in [Mitov, 2011], their redesign and reprogramming for using NLA have to be done after proving the efficiency of NLA realization. Because of this we will concern several types of semi-structured data: ― small datasets - dictionaries, thesauruses, ontologies; ― middle-size and large RDF triple or quadruple datasets, and will provide corresponded experiments and practical implementation. In accordance with this, the PhD research is aimed to propose information model for NL-addressing and corresponded access method as well as the tools for working in such style, theirs main principles, and storing functions. Results presented in this work were implemented in the Institute of Cybernetics V.M. Glushkov at the National Academy of Sciences of Ukraine, Kiev. They had been used for storing dictionaries, thesauruses, ontologies, and RDF-graphs, extracted from multiple documents from own databases as well as from different internet sources.",IVANOVA Krassimira,review:Decision=excluded; review:Time=2025-09-05 15:18,,,2014,0,2025-09-05 15:18,
Industrial Digitally Prototypes,The whole product lifetime can be firstly defined into 5 major phases.,Niemann Jörg; Niemann Jörg; Pisla Adrian; Pisla Adrian,review:Decision=excluded; review:Time=2025-09-05 15:18,,,2021,0,2025-09-05 15:18,
Multi-domain data modeling for biometrics,"Recently, much work has been performed on CBIR (content based image retrieval) that treats images as single data domain. However, in our highly digitized society, information is being supplied in multiple domains where the data is linked across domains. For example, a web site does contain images, but it may also contain text, hyperlinks, documents, sound files, movies, and other domains of data. Performing recall operations within single domains eliminates the possibility of employing cross-domain inferences. In this work, a multi-domain search space is presented in with two domains: speech and facial images. A single search space is created that contains data from these vastly different domains and cross-domain inferences are allowed. In other words, queries in the speech domain can retrieve image data even if there was no hard link between these data samples. Generation of multidomain search spaces will eventually expand CBIR systems to include data from a variety of sources.",Chen Alex; Kinser Jason,Accuracy; Biometrics; Data domains; Data models; data query; Face; Image color analysis; IsoMap; Speech; Vectors; review:Decision=included; review:Time=2025-09-05 15:17,10.1109/AIPR.2011.6176354,,2011,1,2025-09-05 15:17,
""" PaGaNe""–a CLASSIFICATION MACHINE LEARNING SYSTEM BASED on the MULTIDIMENSIONAL NUMBERED INFORMATION SPACES","A classification machine learning system ""PaGaNe"" based on the multidimensional numbered information spaces for memory structuring is presented in the paper. Testing results, which show the efficiency of chosen approach, are presented.",MITOV ILIA; IVANOVA KRASSIMIRA; MARKOV KRASSIMIR; VELYCHKO VITALII; VANHOOF KOEN; STANCHEV PETER,review:Decision=excluded; review:Time=2025-09-05 15:18,,,2010,0,2025-09-05 15:18,
A SURVEY OF MATHEMATICAL AND INFORMATIONAL FOUNDATIONS OF THE BIGARM ACCESS METHOD,"The BigArM is an access method for storing and accessing Big Data. It is under development. In this survey we present its mathematical and informational foundations as well as its requirements to realization characteristics. Firstly, we outline the needed basic mathematical concepts, the Names Sets, and hierarchies of named sets aimed to create a specialized model for organization of information bases called “Multi-Domain Information Model” (MDIM). The “Information Spaces” defined in the model are kind of strong hierarchies of enumerations (named sets). Further we remember the main features of hashing and types of hash tables as well as the idea of “Dynamic perfect hashing” and “Trie”, especially – the “Burst trie”. Hash tables and tries give very good starting point. The main problem is that they are designed as structures in the main memory which has limited size, especially in small desktop and laptop computers. To solve this problem, dynamic perfect hashing and burst tries will be realized as external memory structures in BigArM.",Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:16,,,2015,0,2025-09-05 15:16,
Storing and Processing Accounting Information Based on Collect/Report Paradigm,The possibility to store and process accounting information using the Collect/Report Paradigm (CRP) is outlined in this paper. The main idea consists of using CRP to distribute accounting information in multi-dimensional information spaces and stored and processed it in parallel in cloud. Every account may be presented by a separated layer which contains two named sets – Dr and Cr. Storing and reporting may be provided simultaneously without recompilation of the information base.,Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2015,0,2025-09-05 15:18,
MULTI-VARIANT PYRAMIDAL CLUSTERING AND ANALYSIS HIGH-DIMENSIONAL DATA,In this work an example of multi-variant clustering is presented. The problems to be solved are described and multi-variant clustering based on pyramidal multi-layer multi-dimensional structures is outlined. The conclusion is that the multi-variant clustering combined with pyramidal generalization and pruning gives reliable results.,Markov Krassimir; Ivanova Krassimira B; Velychko Vitalii,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2012,0,2025-09-05 15:17,
Automated translation from domain knowledge to software model: EXCEL2UML in the tunneling domain,"The development of software tools is a collaborative process involving both the domain experts and the software engineers. This requires efficient communication considering different expertise and perspectives. Additionally, the two groups utilize language and communication tools in disparate ways. This, in turn, may lead to hidden misunderstandings in the requirement analysis phase and potentially result in implementation problems later on, that is difficult and costly to correct. In this paper, we demonstrate the above mentioned challenge via a use case from the tunneling domain. In particular, during the requirement analysis phase for a software capable of handling the data model of the subsoil. The domain experts in the field can best express the complexity of their domain by describing its artifacts, which in most cases are incomprehensible to the software engineers. We outline a method that interleaves requirement analysis and software modeling to enable an iterative increase of the accuracy and completeness of the information extracted from those artifacts and integrated into a flexible software model, which can produce testable software code automatically. Furthermore, we present a prototypical implementation of our method and a preliminary evaluation of the approach.",Paskaleva Galina; Mazak-Huemer Alexandra; Villeneuve Marlène; Waldhart Johannes,review:Decision=excluded; review:Time=2025-09-05 15:16,10.36680/j.itcon.2023.019,,2023,0,2025-09-05 15:16,
BUILDING DATA WAREHOUSES USING NUMBERED INFORMATION SPACES,An approach for organizing the information in the data warehouses is presented in the paper. The possibilities of the numbered information spaces for building data warehouses are discussed. An application is outlined in the paper.,Mostovoi Sergey V,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,,0,2025-09-05 15:17,
Smart City Middleware: A Survey and a Conceptual Framework,"Smart city middleware serves as a foundational tool in the evolution of urban digitalization, acting as an intermediary software layer that simplifies the development, deployment, and management of applications tailored for smart urban environments. However, the development of effective middleware for smart cities is challenging. The present research embarks on a comprehensive exploration of the smart city middleware landscape, unraveling the intricacies of its development and the challenges faced therein. Rooted in the assessment of 20 distinct middleware solutions, our study highlights the pivotal technologies, features and functionalities that are imperative for a middleware to effectively support a city’s digital transformation. The functional and non-functional requirements form the nucleus of our evaluation. We also explore the architectural styles pivotal to middleware development and the programming paradigms shaping smart city application development. Our study highlights challenges in using middleware for smart city applications, such as interoperability, scalability, security amidst big data, context management, reliability, quality of service, energy efficiency, and compliance with technological standards and regulations. Based on the detailed analysis, we propose a conceptual framework for smart city middleware, shaped by the challenges and requirements identified in existing literature and middleware solutions. This framework is designed to reflect the diverse demands and complexities of urban digital transformation, and guide smart city middleware development accordingly. As a result, this research stands as a reference study for software developers, urban planners, and researchers, outlining the current state and future directions in the domain of smart city middleware.",Goumopoulos Christos,review:Decision=included; review:Time=2025-09-05 15:16,10.1109/ACCESS.2023.3349376,,2024,1,2025-09-05 15:16,
Building data warehouses using numbered information spaces,An approach for organizing the information in the data warehouses is presented in the paper. The possibilities of the numbered information spaces for building data warehouses are discussed. An application is outlined in the paper.,Mostovoi Sergey V,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,,0,2025-09-05 15:17,
Class association rule mining using multidimensional numbered information spaces,"Data mining is of great importance in the overall process of knowledge discovery. In this dissertation we focused our attention in the part of discoveryoriented methods and especially classification algorithms. Class-Association Rules (CAR) algorithms have a special place within the family of classification algorithms. This type of classifiers offers a number of advantages: efficiency of the training regardless of the training set; easy handling with high dimensionality; very fast classification; high accuracy; classification model easily comprehensible for humans. The main classification workflow of CAR algorithms usually involves three phases: generating the rules, pruning, and recognition. The mining of association rules is a typical data mining task that works in an unsupervised manner. A major advantage of association rules is that they are theoretically capable to reveal all interesting relationships in a database. But for practical applications the number of mined rules is usually too large to be exploited entirely. Hence, a pruning phase is applied in order to build accurate and compact classifiers. The pruning can be applied during preprocessing, simultaneously to the association rules mining, or during post-processing. Different rule quality measures and rule ordering schemes can be applied in the process of rule selection. There are also different options which can be considered for the recognition phase – e.g. to use a simple rule or to use a set of rules with different types of ordering schemas. On the other hand, the process of creating classification models inevitably touches upon the use of appropriate access methods which facilitate access to different kinds of structures used in such algorithms. Our effort had been focused on the memory organization called Multidimensional numbered information spaces which allows to operate with contextfree multidimensional data structures. The program realization of such structures is named ArM 32. Multi-Domain Information Model (MDIM) and respectively Arm 32 are based on the process of replacing names by numbers which allows to use mathematical functions and addressing vectors for accessing the information. Our approach is to use such structures and operations in the implementation of one class association rule classifier in order to provide evidence on the vitality of the idea of using context-free multidimensional data structures and direct access as a powerful tool for knowledge discovery. We have proposed two classification algorithms – Pyramidal Growing Networks (PGN) and Multi-layer Pyramidal Growing Networks (MPGN). PGN creates association rules, optimized for maximal accuracy of produced rules. One of the main characteristics of PGN is that it is a parameter-free classifier. The association rule mining is executed from the longest rules to the shorter ones until no intersections between patterns in the classes are possible. In the pruning phase the contradictions and inconsistencies of more general rules are cleared, after that the pattern set is compacted excluding all more concrete rules within the classes. PGN is introduced as a useful tool for questioning the support-first principle used by many associative classifiers when mining for association rules. PGN reverses the common approach and focuses primarily on the confidence of the association rules and only in a later stage on the support of the rules. The main purpose is twofold: to provide a proof of concept for this new approach and to gather evidence on its potential. MPGN is based on multilayer structure. It involves possibility to escape combinatorial explosion using smart disposing of the information in the multilayer structures called ""pyramids"". These structures can be easily implemented using ArM-structures. These algorithms are implemented in the data mining environment PaGaNe, developed by the team from the Institute of Mathematics and Informatics – Bulgarian Academy of Sciences; Iliya Mitov and Krassimira Ivanova are the principal developers. PaGaNe incorporates different types of statistical analysis methods, discretization algorithms, association rule miner, as well as classification algorithms, which all are based on the use of multi-dimensional numbered information spaces. The Lenses dataset is used as a test example to illustrate the specifics of the proposed algorithms, the process of creating classification models as well as the process of recognition. We demonstrate that PGN produces the pattern set that is both minimal and complete for covering the learning set, which is an indicator for expectation that PGN will produce tight model and good accuracy results. In the case of MPGN we have demonstrated the process of creating main construction elements. We also have illustrated the functionality which allows to visualize how the pyramids are being created and how the queries are being recognized. We carried out experiments with 25 datasets from the UCI machine learning repository [Frank and Asuncion, 2010]. The experiments had been conducted using the data mining environment PaGaNe, the knowledge analysis system Weka, and LUCS-KDD Repository. A comparison between PGN, MPGN and some other CAR algorithms, as well as decision tree and decision rule classifiers which have similar behavior of creating the task model, had been done. One series of experiments aimed to study what accuracy had been obtained while preprocessing real data with different discretizators realized in PaGaNe. We found that in general PGN-classifier trained on data preprocessed by Chimerge with 95trained on data preprocessed by the other discretization methods. The main reason for this is that using Chi-square statistical measure as criterion for class dependency in adjacent intervals of a feature results in good separation between class labels. A second set of experiments studied the process of growing the learning sets and how this reflects on the classification model and the accuracy of PGN and MPGN; more specifically, we studied the critical point of the amount of the learning set in which classification model is relatively compact and the received accuracy stabilizes. Of course this critical point highly depends on the choice of dataset. A third set of experiments were focused on analyzing different exit points of MPGN. The received results showed that in a lot of cases the build constructs lead to excluding only one class as best competitor. Other cases usually fall into competition between classes, where different strategies for ordering the competitors can be applied. A very few cases fall into the way where MPGNalgorithm did not work and alternative choice is given. A fourth set of experiments aimed to analyze the dependencies of classifiers' behaviors when the noise rush in the dataset attributes; for this set we used the Monks1 dataset. The experiments demonstrated that noising in the dataset worsens considerably the accuracy of PGN which had been designed to perform well in clear datasets. However, experiments with other existing classifiers showed that they also were not been able to resist noising attacks. We made the comparison of overall accuracy between PGN, MPGN (with two recognition strategies – S1 and S2), CMAR, OneR, JRip, J48 and REPTree. The Friedman test showed statistical difference between tested classifiers. The post-hoc Nemenyi test showed that our PGN has best overall performance between examined classifiers and MPGN is competitive with CMAR, J48, JRip and REPTree. The experimental results are very positive and show that PGN is competitive with classification methods that build similar classification behavior. At the same time, it has an essential advantage over the other classifiers being parameter free. Furthermore, the empirical results showed that PGN is slightly more sensitive to noise than techniques such as C4.5 and RIPPER. However, its overall accuracy was still very good compared to these classifiers. In general, the results provide evidence that the confidence-first approach yields interesting opportunities for knowledge discovery.",MITOV Iliya,review:Decision=excluded; review:Time=2025-09-05 15:17,,,2011,0,2025-09-05 15:17,
Multi-variant pyramidal clustering and analysis high-dimensional data,In this work an example of multi-variant clustering is presented. The problems to be solved are described and multi-variant clustering based on pyramidal multi-layer multi-dimensional structures is outlined. The conclusion is that the multi-variant clustering combined with pyramidal generalization and pruning gives reliable results.,Markov Krassimir; Ivanova Krassimira B; Velychko Vitalii,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2012,0,2025-09-05 15:17,
Evaluating SPARQL-based model checking: Potentials and limitations,"Model checking is an important task in the BIM collaboration process to prevent expensive planning errors. The submodels of the individual disciplines are transferred into a coordination model. Part of the transfer is a conversion into an exchange format. The exchange format allows the import into the model checking application. In the model checking application routines are performed to check the model against collisions and building regulation violations. During the transfer into the exchange format, information may get lost, especially with parameters that are not yet part of the exchange format supported by the authoring software. In recent years, ontologies have been investigated as a feasible approach to combine the submodels, since they model data in a flexible manner. Hence in the conversion process to an application-specific ontology, the data structure of the submodels can widely persist, which could lead to smaller information loss in comparison to converting the data into a standardized exchange format. The evaluation of the geometric properties of the building is indispensable for detecting and analyzing collisions. The basis for the connection of the different sub models could be the BOT (Building Topology Ontology), which defines the topological structure of a building and can be used to represent further building information by linking it with other ontologies. The relevant geometric relationships for the collision model checks have to be derived with a geometry kernel. For the research in this paper pythonOCC, a wrapper for the geometry kernel Open CASCADE is used with the Semantic Web's own query language SPARQL, queries can be formulated to analyze the collision relationships in combination with other semantic information. These queries can be used to verify model correctness. By connecting the information from different domains, more sophisticated tests are possible than in an exchange format dependent model checking application. The goal is to integrate the developed functionalities into a project platform. This platform is based on an extensive project description in an ontology-based data model and is connected to different authoring tools for the exchange of information.",Hoffmann A; Shi M; Wagner A; Thiele C-D; Huyeng T-J; Rüppel U; Sprenger W,review:Decision=included; review:Time=2025-09-05 15:17,10.1201/9781003191476-11,,2021,1,2025-09-05 15:17,
From concept to reality: a contemporary framework for virtual designing and commissioning processing plant systems,"The design and commissioning of process systems involve complex procedures often guided by digital Mock-Ups, replacing physical prototypes. This study presents a comprehensive framework integrating Virtual Commissioning, 3D CAD modeling, and industry standards to enhance the design process of such systems, supporting first-time-right implementations. The methodology combines literature review, industry standards, and case studies from the feed processing industry. The framework emphasizes interoperability, data model integrity, simulation, and cross-disciplinary communication. Findings suggest a refined model structure for modern manufacturing complexities, with practical implications for industry practitioners. Future research should refine the framework and explore broader industrial applications, improving implementation processes.",Nielsen Mads Kjærgaard; Beliatis Michail J.; Tambo Torben,review:Decision=excluded; review:Time=2025-09-05 15:16,10.1016/j.procs.2025.01.159,,2025,0,2025-09-05 15:16,
An Approach to Multi-Domain Data Model Development Based on the Model-Driven Architecture and Ontologies,"To date, there are many diverse data representation technologies (EDIFACT, XML, JSON, CSV, relational model, NoSQL). Transition to new technologies or the integration of information systems based on different technological stacks is a complex and expensive process. Platform-independent models take an important role in this process. The structure of such a model is described in this article. However, given the data model has been created at the junction of different domains, it may be not enough. In such case, a one more step of abstraction and a movement to the computation-independent model is required. The authors propose to create it in an ontological form.",Nikiforov Denis A; Lisikh Igor G; Sivakov Ruslan L,⛔ No DOI found; review:Decision=included; review:Time=2025-09-05 15:16,,,2015,1,2025-09-05 15:16,
Increased efficiency in virtual commissioning with automated model generation based on component libraries,"The use of Virtual Commissioning (VC) is becoming increasingly relevant for the engineering of automation systems. VC allows for validating concepts and design solutions in the early phases of product development, thereby improving communication among disciplines, reducing the costs for bug fixing and accelerating the overall engineering process, in particular the commissioning of plants and machines. However, as an additional software tool in engineering, the introduction of VC causes initial training and more importantly continuous extra modelling efforts. An approach to overcome this flaw is the automated generation of VC models based on existing engineering information from the system or domain level – ideally in tandem with the reuse of solution models from previous project. The latter can prove particularly beneficial in the context of modular building-block systems as often pursued in automation engineering.",Pyschny Nicolas; Rudat Ben; Permin Eike,review:Decision=excluded; review:Time=2025-09-05 15:16,10.1016/j.procir.2022.05.258,,2022,0,2025-09-05 15:16,
no doi title,,,review:Decision=excluded; review:Time=2025-09-07 10:05; review:Reason=out of scope,,,2021,0,2025-09-07 10:05,out of scope
THE B-TERMINAL BUSY PROBABILITY PREDICTION,"In the teletraffic engineering of all the telecommunication networks, parameters characterizing the terminal traffic are used. One of the most important of them is the probability of finding the called (B-terminal) busy. This parameter is studied in some of the first and last papers in Teletraffic Theory. We propose a solution in this topic in the case of (virtual) channel systems, such as PSTN and GSM. We propose a detailed conceptual traffic model and, based on it, an analytical macro-state model of the system in stationary state, with: Bernoulli–Poisson–Pascal input flow; repeated calls; limited number of homogeneous terminals; losses due to abandoned and interrupted dialling, blocked and interrupted switching, not available intent terminal, blocked and abandoned ringing and abandoned conversation. Proposed in this paper approach may help in determination of many network traffic characteristics at session level, in performance evaluation of the next generation mobile networks.",Poryazov Stoyan,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,,0,2025-09-05 15:17,
Storing RDF graphs using NL-addressing,"NL-addressing is a possibility to access information using natural language words as addresses of the information stored in the multi-dimensional numbered information spaces. For this purpose the internal encoding of the letters is used to generate corresponded co-ordinates. The tool for working in such style is named OntoArM. Its main principles, functions and using for storing RDF graphs are outlined in this paper",Velychko Vitalii; IVANOVA Krassimira; MARKOV Krassimir,⛔ No DOI found; NL-addressing; ontology representations.; RDF graphs; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2012,0,2025-09-05 15:18,
Data independence in the multi-dimensional numbered information spaces,The concept of data independence designates the techniques that allow data to be changed without affecting the applications that process it. The different structures of the information bases require corresponded tools for supporting data independence. A kind of information bases (the Multi-dimensional Numbered Information Spaces) are pointed in the paper. The data independence in such information bases is discussed.,Markov Krassimir,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2008,0,2025-09-05 15:17,
,,,,,,,,,
has doi,,,review:Decision=included; review:Time=2025-09-07 10:00; review:Reason=looks relevant,,,2020,1,2025-09-07 10:00,looks relevant
,,,,,,,,,
Building data warehouses using numbered information spaces,An approach for organizing the information in the data warehouses is presented in the paper. The possibilities of the numbered information spaces for building data warehouses are discussed. An application is outlined in the paper.,Markov Krassimir,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2006,0,2025-09-05 15:17,
,,,,,,,,,
Decentralization in industry 4.0 supported by opc ua multi-domain information models: Case study and implementation challenges,"This paper introduces an OPC UA multi-domain information model aimed at supporting decentralization in industrial automation and includes a case study addressing the implementation challenges associated with integrating standards such as Device, PackML, ISA95, and AAS. The developed model enables interoperability and harmonization of information models across various domains, which is crucial for Industry 4.0 applications. The work highlights the paradox of decentralization, where independent modules require centralized control and data coordination, necessitating a unified information model. Key contributions include the proposal of methods to address technical challenges related to data type compatibility and inheritance of properties across domains. Additionally, the text emphasizes the importance of system integration for data acquisition from physical assets. A comparative analysis of implementation on PLCs, on-premises computing platforms, and cloud environments highlights tradeoffs in deployment depending on system complexity and response time requirements. The study results may assist in developing an information model for a decentralized unit in Industry 4.0.",Pribiš Rudolf; Beňo Lukáš; Pajpach Martin; Drahoš Peter; Kocák Ondrej,Interoperability; multi-domain information model; Data models; Computational modeling; decentralization; Fourth Industrial Revolution; Industry 4.0; OPC UA; Proposals; Servers; Standards; Synchronization; System integration; Time factors; review:Decision=included; review:Time=2025-09-05 15:16; review:Reason=Om een reden,10.1109/KI64036.2025.10916421,,2025,1,2025-09-05 15:16,Om een reden
Urban flooding digital twin system framework,"Digital twin-based resilience management systems are essential for improving urban flooding resilience, enabling lifecycle-oriented emergency management as a continuous process rather than discrete phases. However, to the best of our knowledge, currently, there lacks a system framework for supporting the development of an urban flooding digital twin (UFDT) platform capable of managing urban flooding events across the entire management lifecycle. This UFDT framework is required to (1) accommodate UFDT model-generation methods/tools for preparing what-if scenarios, (2) support readiness simulations and evaluations, (3) coordinate various collaborative prevention and intervention services from different stakeholders in response to any flooding emergence, and (4) monitor and forecast the flooding risks in the recovery phase. To fill this gap, in this paper, a new UFDT system framework is developed based on a user-centred product design process with the consideration of the above requirements. It has two key components: a UFDT conceptual model, and a generative methodology for its rapid construction/updating/adaptation to varying levels of detail. A framework prototype has been developed for testing the conceptual model at city, regional and street levels and exemplar generative methods/tools to assess the framework’s ability and potential to provide scalable, adaptable, and stakeholder-focused solutions to urban flooding resilience management[Q1].",Ge Chenyu; Qin Shengfeng,review:Decision=included; review:Time=2025-09-05 15:16,10.1080/21642583.2025.2460432,,2025,1,2025-09-05 15:16,
,,,,,,,,,
About NL-addressing,"В настоящей работе представлена идея естественно-языковой адресации. Это дополнительная возможность для представления онтологической информации в интеллектуальных системах. Естественно-языковая адресация имеет ряд преимуществ. На первом месте – это линейная алгоритмическая сложность, которая зависит от максимальной длины слов (max_L), а не от их количества. Во-вторых, это уменьшение объема занимаемой памяти – дополнительные индексы не используются. В-третьих, уменьшение времени обработки из-за полного отсутствия поиска – информация извлекается прямо по адресу. Необходимо отметить, что это универсальное представление информации одновременно доступной как для человека, так и для автоматизированных систем. Такой способ организации информации применим для ее хранения и использования в библиотеках онтологий, терминов, понятий, текстовых документов",Velychko Vitalii; IVANOVA Krassimira; MARKOV Krassimir,⛔ No DOI found; Естественно-языковая адресация; организация онтологических баз данных; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2012,0,2025-09-05 15:18,
Storing RDF graphs using NL-addressing,"NL-addressing is a possibility to access information using natural language words as addresses of the information stored in the multi-dimensional numbered information spaces. For this purpose the internal encoding of the letters is used to generate corresponded co-ordinates. The tool for working in such style is named OntoArM. Its main principles, functions and using for storing RDF graphs are outlined in this paper",Velychko Vitalii; IVANOVA Krassimira; MARKOV Krassimir,NL-addressing; ontology representations.; RDF graphs; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2012,0,2025-09-05 15:18,
,,,,,,,,,
,,,,,,,,,
Multi-domain data modeling for biometrics,"Recently, much work has been performed on CBIR (content based image retrieval) that treats images as single data domain. However, in our highly digitized society, information is being supplied in multiple domains where the data is linked across domains. For example, a web site does contain images, but it may also contain text, hyperlinks, documents, sound files, movies, and other domains of data. Performing recall operations within single domains eliminates the possibility of employing cross-domain inferences. In this work, a multi-domain search space is presented in with two domains: speech and facial images. A single search space is created that contains data from these vastly different domains and cross-domain inferences are allowed. In other words, queries in the speech domain can retrieve image data even if there was no hard link between these data samples. Generation of multidomain search spaces will eventually expand CBIR systems to include data from a variety of sources.",Chen Alex; Kinser Jason,Accuracy; Biometrics; Data domains; Data models; data query; Face; Image color analysis; IsoMap; Speech; Vectors; review:Decision=included; review:Time=2025-09-05 15:17,10.1109/AIPR.2011.6176354,,2011,1,2025-09-05 15:17,
Multi-layer knowledge representation,"An approach for knowledge representation based on post-relation type of information bases is outlined in the paper. Explanation starts with remembering the idea of Natural Language Addressing. After that, the idea of Multi-layer Knowledge Representation by Means of Natural Language Addressing is presented.",Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2014,0,2025-09-05 15:18,
Multi-domain information model,"The “Multi-Domain Information Model” (MDIM) has been established twenty years ago. For a long period it has been used as a basis for organisation of various information bases. The first publication containing some details of MDIM is [Markov, 1984] but the model has not been fully presented till now. In addition, over the years, the model has been extended with some new concepts like “information space”, “metaindex”, “polyindexation”, etc. which we will introduce in this paper. The present paper aims to present MDIM as a coherent whole.",Markov Krassimir,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2004,0,2025-09-05 15:17,
The b-terminal busy probability prediction,"In the teletraffic engineering of all the telecommunication networks, parameters characterizing the terminal traffic are used. One of the most important of them is the probability of finding the called (B-terminal) busy. This parameter is studied in some of the first and last papers in Teletraffic Theory. We propose a solution in this topic in the case of (virtual) channel systems, such as PSTN and GSM. We propose a detailed conceptual traffic model and, based on it, an analytical macro-state model of the system in stationary state, with: Bernoulli–Poisson–Pascal input flow; repeated calls; limited number of homogeneous terminals; losses due to abandoned and interrupted dialling, blocked and interrupted switching, not available intent terminal, blocked and abandoned ringing and abandoned conversation. Proposed in this paper approach may help in determination of many network traffic characteristics at session level, in performance evaluation of the next generation mobile networks.",Poryazov Stoyan,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,,0,2025-09-05 15:17,
,,,,,,,,,
Advance of the access methods,The goal of this paper is to outline the advance of the access methods in the last ten years as well as to make review of all available in the accessible bibliography methods.,Markov Krassimir; Ivanova Krassimira; Karastanev Stefan; Mitov Ilia,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2008,0,2025-09-05 15:17,
,,,,,,,,,
About two stereotypes of uml profile for multidimensional numbered data bases,"Two stereotypes for Numbered Space (NSpace) and Space with Natural Language Addressing (NLASpace) from an UML profile for modeling Multi-Dimensional Numbered Data Bases (MDNDB™) are shortly outlined in this paper. MDNDB™ is a tool for storing Big Data locally as well as in the cloud. Because of this, it is important to have appropriate modeling language to support the design of practical applications.",Markov Krassimir; Ivanova Krassimira; Velychko Vitalii; Chebanyuk Olena,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2019,0,2025-09-05 15:17,
About two stereotypes of uml profile for multidimensional numbered data bases,"Two stereotypes for Numbered Space (NSpace) and Space with Natural Language Addressing (NLASpace) from an UML profile for modeling Multi-Dimensional Numbered Data Bases (MDNDB™) are shortly outlined in this paper. MDNDB™ is a tool for storing Big Data locally as well as in the cloud. Because of this, it is important to have appropriate modeling language to support the design of practical applications.",Markov Krassimir; Ivanova Krassimira; Velychko Vitalii; Chebanyuk Olena,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2019,0,2025-09-05 15:17,
,,,,,,,,,
,,,,,,,,,
Smart city middleware: a survey and a conceptual framework,"Smart city middleware serves as a foundational tool in the evolution of urban digitalization, acting as an intermediary software layer that simplifies the development, deployment, and management of applications tailored for smart urban environments. However, the development of effective middleware for smart cities is challenging. The present research embarks on a comprehensive exploration of the smart city middleware landscape, unraveling the intricacies of its development and the challenges faced therein. Rooted in the assessment of 20 distinct middleware solutions, our study highlights the pivotal technologies, features and functionalities that are imperative for a middleware to effectively support a city’s digital transformation. The functional and non-functional requirements form the nucleus of our evaluation. We also explore the architectural styles pivotal to middleware development and the programming paradigms shaping smart city application development. Our study highlights challenges in using middleware for smart city applications, such as interoperability, scalability, security amidst big data, context management, reliability, quality of service, energy efficiency, and compliance with technological standards and regulations. Based on the detailed analysis, we propose a conceptual framework for smart city middleware, shaped by the challenges and requirements identified in existing literature and middleware solutions. This framework is designed to reflect the diverse demands and complexities of urban digital transformation, and guide smart city middleware development accordingly. As a result, this research stands as a reference study for software developers, urban planners, and researchers, outlining the current state and future directions in the domain of smart city middleware.",Goumopoulos Christos,review:Decision=included; review:Time=2025-09-05 15:16,10.1109/ACCESS.2023.3349376,,2024,1,2025-09-05 15:16,
Managerial and ontological issues in the development of enterprise architecture: Experiences from a case study,"With increasing numbers and complexity of information models, it is becoming more and more important to have supporting modeling frameworks. Previous literature within this field has been descriptive, conceptual, and normative with few empirical studies addressing managerial and cognitive challenges during the development of large-scale models. This article addresses modeling from three perspectives: Enterprise Architecture (EA), Ontology, and Management Theory. The article presents the experiences from a large corporation effort to develop a modeling framework. The focus is on managerial and cognitive issues related to the development and use of the modeling framework. The conclusion is that large-scale modeling activities are complex involving several cognitive issues, such as managing different interpretations of terms due to different languages, modeling frameworks, cultures, and information systems.",Hedman Jonas; Schonström Mikael,⛔ No DOI found; review:Decision=included; review:Time=2025-09-05 15:12,,,2013,1,2025-09-05 15:12,
Multi-domain information model,"The “Multi-Domain Information Model” (MDIM) has been established twenty years ago. For a long period it has been used as a basis for organisation of various information bases. The first publication containing some details of MDIM is [Markov, 1984] but the model has not been fully presented till now. In addition, over the years, the model has been extended with some new concepts like “information space”, “metaindex”, “polyindexation”, etc. which we will introduce in this paper. The present paper aims to present MDIM as a coherent whole.",Markov Krassimir,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2004,0,2025-09-05 15:17,
Industrial digitally prototypes,The whole product lifetime can be firstly defined into 5 major phases.,Niemann Jörg; Niemann Jörg; Pisla Adrian; Pisla Adrian,review:Decision=excluded; review:Time=2025-09-05 15:18,10.1007/978-3-030-56449-0_17,,2021,0,2025-09-05 15:18,
,,,,,,,,,
Natural Language Addressing,"Large unstructured or semi-structured datasets require a high level of computational sophistication because operations that are easy at a small scale — such as moving data betweenmachines or in and out of storage, visualizing the data, or displaying results —can all require substantial algorithmic ingenuity. As a data set becomes increasingly massive, it may be infeasible to gather it in one place and analyze it as a whole. Thus, there may be a need for algorithms that operate in a distributed fashion, analyzing subsets of the data and aggregating those results to understand the complete set. One aspect of this is the challenge of data assimilation, in which we wish to use new data to update model parameters without reanalyzing the entire data set. This is essential when new waves of data continue to arrive, or subsets are analyzed in isolation of one another, and one aims to improve the model and inferences in an adaptive fashion — for example, with streaming algorithms [NRC, 2013].",Markov Krassimir; Ivanova Krassimira; Vanhoof Koen; Velychko Vitalii,review:Decision=excluded; review:Time=2025-09-05 15:17,,,2015,0,2025-09-05 15:17,
,,,,,,,,,
Storing data using natural language addressing,"Large unstructured or semi-structured datasets require a high level of computational sophistication because operations that are easy at a small scale — such as moving data between machines or in and out of storage, visualizing the data, or displaying results —can all require substantial algorithmic ingenuity. As a data set becomes increasingly massive, it may be infeasible to gather it in one place and analyze it as a whole. Thus, there may be a need for algorithms that operate in a distributed fashion, analyzing subsets of the data and aggregating those results to understand the complete set. One aspect of this is the challenge of data assimilation, in which we wish to use new data to update model parameters without reanalyzing the entire data set. This is essential when new waves of data continue to arrive, or subsets are analyzed in isolation of one another, and one aims to improve the model and inferences in an adaptive fashion — for example, with streaming algorithms [NRC, 2013]. In accordance with the actuality of these problems, firstly in [Mitov, 2011] and after that in [Markov et al, 2013] a new idea has been proposed. It is a method for effective building and storing of pattern sets in multi-layer structures during the process of associative rule mining using the possibilities of multi-dimensional numbered information spaces. The main algorithm was called “MPGN”, an abbreviation from ""Multi-layer Pyramidal Growing Networks of information spaces"". The main goal was to extend the possibilities of network structures by using a special kind of multi-layer memory structures called ""pyramids"", which permits defining and realizing new opportunities. The bottleneck of MPGN became the need to search in billions of values of the association rules’ features to convert instances in numbered arrays (vectors). This is a part of preprocessing step of algorithm (see page 97 of [Mitov, 2011]). The process of numbering took considerable time. After numbering, the MPGN algorithm had shown very good results. This work is aimed to propose a solution of the problem of searching in big index structures by proposing a special kind of hashing, so-called “multi-layer hashing”, i.e. by implementing recursively the same specialized hash function to build and resolve the collisions in hash tables. In other words, the main idea consists in using the specialized hashing functions in depth till it is needed. This approach is called “Natural Language Addressing” (NLA) [Ivanova et al, 2012a; Ivanova et al, 2013a; Ivanova et al, 2013d]. The common sense meaning of the concept “address” is such as a description of the location (of a person or organization), as written or printed on mail as directions for delivery [AHD, 2009]; the conventional form by which the location of a building is described [Collins, 2003]; a sign in front of a house or business carrying the conventional form by which its location is described; [WordNet, 2012]. We will use the concept “address” in the sense accepted in the Computer Science: the code that identifies where a piece of information is stored [WordNet, 2012]; a name or number used in information storage or retrieval that is assigned to a specific memory location; the memory location identified by this name or number [AHD, 2009]. Natural Language Addressing (NLA) is a possibility to access information using natural language words as paths to the information. For this purpose the internal encoding of the letters is used to generate corresponded path. The idea of Natural Language Addressing (NL-Addressing) is very simple. It is based on the computer internal representation of the word as strings of codes in a system of encoding (ASCII, UNICODE, etc.). For example, the ASCII encoding of the word „accession” has the next representation: (97, 99, 99, 101, 115, 115, 105, 111, 110). It may be used as array for multi-layer hashing, which indicates a path to point, where the corresponded information may be stored. The main problem in such approach is that the words have different lengths and, in addition, several words may form one phrase and this way to be assumed as single concept. This means that we need tools for managing multi-layer hashing with variable path lengths in an integrated structure. Due to the complexity of MPGN algorithm and the corresponded program system realized in [Mitov, 2011], their redesign and reprogramming for using NLA have to be done after proving the efficiency of NLA realization. Because of this we will concern several types of semi-structured data: ― small datasets - dictionaries, thesauruses, ontologies; ― middle-size and large RDF triple or quadruple datasets, and will provide corresponded experiments and practical implementation. In accordance with this, the PhD research is aimed to propose information model for NL-addressing and corresponded access method as well as the tools for working in such style, theirs main principles, and storing functions. Results presented in this work were implemented in the Institute of Cybernetics V.M. Glushkov at the National Academy of Sciences of Ukraine, Kiev. They had been used for storing dictionaries, thesauruses, ontologies, and RDF-graphs, extracted from multiple documents from own databases as well as from different internet sources.",IVANOVA Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2014,0,2025-09-05 15:18,
Storing and processing accounting information based on collect/report paradigm,The possibility to store and process accounting information using the Collect/Report Paradigm (CRP) is outlined in this paper. The main idea consists of using CRP to distribute accounting information in multi-dimensional information spaces and stored and processed it in parallel in cloud. Every account may be presented by a separated layer which contains two named sets – Dr and Cr. Storing and reporting may be provided simultaneously without recompilation of the information base.,Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2015,0,2025-09-05 15:18,
,,,,,,,,,
OntoArM-a system for storing ontologies by natural language addressing,In this paper we present results from experiments for storing RDF ontologies by means of Natural Language Addressing. For experiments we have realized system OntoArM aimed to store RDF triples in multilayer hash tables (information spaces with variable size). The main features of system OntoArM are outlined in the paper. Analysis of the experimental results concluded the work.,Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2014,0,2025-09-05 15:18,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
,,,,,,,,,
Using domain-specific models to facilitate model-based systems-engineering: Development process design modeling with OPM and PROVE,"Model-based Systems Engineering (MBSE) approaches are a step forward in the evolution of computer-aided engineering, and yet, they often incorporate deﬁciencies that may jeopardize their practical utility and usability, as well as the validity of the resulting models. We demonstrate how a domain-speciﬁc modeling approach can relieve some hurdles in adopting MBSE, and how it can be used in tandem with a general-purpose modeling approach to augment and introduce rigor to models. Speciﬁcally, we demonstrate the consequences of theoretical issues that were previously identiﬁed in Object Process Methodology and suggest an approach to solve them. We use a generalized casestudy—derived from extensive process modeling in both academia and industry—to show that a domain-speciﬁc model can signiﬁcantly relax the user’s modeling effort. This demonstration is based on two quantitative metrics: the number of representational elements and available modeling tactics. We discuss the contribution of our approach to model quality, particularly with respect to its rigor and communicability.",Shaked Avi; Reich Yoram,review:Decision=included; review:Time=2025-09-05 15:15,10.3390/app11041532,,2021,1,2025-09-05 15:15,
Urban flooding digital twin system framework,"Digital twin-based resilience management systems are essential for improving urban flooding resilience, enabling lifecycle-oriented emergency management as a continuous process rather than discrete phases. However, to the best of our knowledge, currently, there lacks a system framework for supporting the development of an urban flooding digital twin (UFDT) platform capable of managing urban flooding events across the entire management lifecycle. This UFDT framework is required to (1) accommodate UFDT model-generation methods/tools for preparing what-if scenarios, (2) support readiness simulations and evaluations, (3) coordinate various collaborative prevention and intervention services from different stakeholders in response to any flooding emergence, and (4) monitor and forecast the flooding risks in the recovery phase. To fill this gap, in this paper, a new UFDT system framework is developed based on a user-centred product design process with the consideration of the above requirements. It has two key components: a UFDT conceptual model, and a generative methodology for its rapid construction/updating/adaptation to varying levels of detail. A framework prototype has been developed for testing the conceptual model at city, regional and street levels and exemplar generative methods/tools to assess the framework’s ability and potential to provide scalable, adaptable, and stakeholder-focused solutions to urban flooding resilience management[Q1].",Ge Chenyu; Qin Shengfeng,review:Decision=included; review:Time=2025-09-05 15:16,10.1080/21642583.2025.2460432,,2025,1,2025-09-05 15:16,
,,,,,,,,,
Building data warehouses using numbered information spaces,An approach for organizing the information in the data warehouses is presented in the paper. The possibilities of the numbered information spaces for building data warehouses are discussed. An application is outlined in the paper.,Markov Krassimir,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2006,0,2025-09-05 15:17,
""" PaGaNe""–a CLASSIFICATION MACHINE LEARNING SYSTEM BASED on the MULTIDIMENSIONAL NUMBERED INFORMATION SPACES","A classification machine learning system ""PaGaNe"" based on the multidimensional numbered information spaces for memory structuring is presented in the paper. Testing results, which show the efficiency of chosen approach, are presented.",MITOV ILIA; IVANOVA KRASSIMIRA; MARKOV KRASSIMIR; VELYCHKO VITALII; VANHOOF KOEN; STANCHEV PETER,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2010,0,2025-09-05 15:18,
Example of multi-layer knowledge representation by means of natural language addressing,An approach for knowledge representation based on post-relation type of information bases is outlined in the paper. The idea of Natural Language Addressing and based on it idea of Multi-layer Knowledge Representation are presented.,Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2014,0,2025-09-05 15:18,
,,,,,,,,,
Big data addressed in natural language,"Actions related to the cloud processing of large volumes of semi- or unstructured, streaming data (so-called “Big Data”) pose challenges that organizations must address. They are related to the location of the data in the cloud, its storage and management. Much of Big Data is collected from sources that are external to the business organization, such as social media, demographics, web data, events, news sources, and more. In this article, we discuss an access method to enable support for very large Big Data databases. It is based on the capabilities of Natural Language Addressing (NLA). An important advantage of NLA is the reduction of the amount of occupied memory due to the complete absence of additional indexes, absolute addresses, pointers and additional files, as well as the reduction of processing time due to the complete absence of searches - data is stored / retrieved to / from a direct address.",Karastanev Stefan; Markova Vera; Ivanova Krasimira,Data models; Big Data; big data databases; BigNLA access method; Memory management; natural language addressing; Natural languages; Organizations; Social networking (online); Task analysis; review:Decision=excluded; review:Time=2025-09-05 15:17,10.1109/AEIS61544.2023.00016,,2023,0,2025-09-05 15:17,
Distributed virtual laboratories for smart sensor system design,"In the article it is considered preconditions and main principles of creation of virtual laboratories for computer-aided design, as tools for interdisciplinary researches. An important feature of this project is using the advanced multi-dimensional access method for organizing the information base of the Virtual laboratory. Virtual laboratory, what are offered, is worth to be used on the stage of the requirements specification or EFT-stage, because it gives the possibility of fast estimating of the project realization, certain characteristics and, as a result, expected benefit of its applications. Using of these technologies already increase automation level of design stages of new devices for different purposes. Proposed computer technology gives possibility to specialists from such scientific fields, as chemistry, biology, biochemistry, physics etc, to check possibility of device creating on the basis of developed sensors. It lets to reduce terms and costs of designing of computer devices and systems on the early stages of designing, for example on the stage of requirements specification or EFT-stage. An instance of using the VLCAD is designing the Portable Device"" Floratest"" as Tool for Estimating of Megalopolis Ecology State. Portable device"" Floratest"" is aimed for express-diagnostic of plant state. It is developed in the VM Glushkov Institute of Cybernetics of National Academy of Sciences of Ukraine. Party of this device is manufactured and transferred to organizations, worked in the agricultural sector, environmental protection area, mineral fertilizer production etc. for working out of methodical tools. Using of the device for estimating of megalopolis ecology state by means of evaluation of green plant state is described in the article. Together with Megalopolis Ecomonitoring and Biodiversity Research Center of National Academy of Sciences of Ukraine there were got results of experimental researches of influence detecting of heavy metals and harmful substances on the trees and plants in Kiev.",Velychko Vitalii; Palagin Oleksandr; Romanov Volodymyr; Galelyuka Igor; Fedak Volodymyr; Grusha Volodymyr; Artemenko Dmytro; Galelyuka Oksana,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2009,0,2025-09-05 15:17,
,,,,,,,,,
From Concept to Reality: A Contemporary Framework for Virtual Designing and Commissioning Processing Plant Systems,"The design and commissioning of process systems involve complex procedures often guided by digital Mock-Ups, replacing physical prototypes. This study presents a comprehensive framework integrating Virtual Commissioning, 3D CAD modeling, and industry standards to enhance the design process of such systems, supporting first-time-right implementations. The methodology combines literature review, industry standards, and case studies from the feed processing industry. The framework emphasizes interoperability, data model integrity, simulation, and cross-disciplinary communication. Findings suggest a refined model structure for modern manufacturing complexities, with practical implications for industry practitioners. Future research should refine the framework and explore broader industrial applications, improving implementation processes.",Nielsen Mads Kjærgaard; Beliatis Michail J.; Tambo Torben,review:Decision=excluded; review:Time=2025-09-05 15:16,10.1016/j.procs.2025.01.159,,2025,0,2025-09-05 15:16,
Multi-domain virtual network embedding with coordinated link mapping,"Network Virtualization, which allows the coexistence of various logical networks on shared physical infrastructure, has become popular in recent years. The optimal mapping of virtual resource to physical resource is a major issue in network virtualization. This problem, called virtual network embedding (VNE), has been well explored in the context of one physical domain, which is in practice operated by a single infrastructure provider (InP). However, the needs of virtual network (VN) is rapidly growing, and quite a number of VNs have to be established across multi-domain. For multi-domain VNE, infrastructure providers can no longer just solve their own single domain VNE problem, but have to cooperate to build the whole VN. Therefore, new challenge arises for the multi-domain VNE, compared to traditional single domain VNE. The existing investigations on this problem mainly focus on decomposing a VN to sub VN for each domain, but little attention has been paid to the joint relation between intra-domain and inter-domain (peering) links. In this paper, we propose a multi-domain link mapping method which combines the intra and peering link mapping so as to optimize the overall resource utilization. Our approach is easy to be deployed since it is based on current Internet architecture. Evaluation shows that our approach brings improvements related to existing methods.",Li Shuopeng; Saidi Mohand Yazid; Chen Ken,Bandwidth; III-V semiconductor materials; Indium phosphide; Peer-to-peer computing; Substrates; Topology; Virtualization; review:Decision=excluded; review:Time=2025-09-05 15:17,10.1109/SOFTCOM.2016.7772158,,2016,0,2025-09-05 15:17,
PDF,,,,,file:///Users/arjen/Zotero/storage/QNRRIAAA/Nikiforov e.a. - 2015 - An Approach to Multi-Domain Data Model Development Based on the Model-Driven Architecture and Ontolo.pdf,,,,
Automated translation from domain knowledge to software model: EXCEL2UML in the tunneling domain,"The development of software tools is a collaborative process involving both the domain experts and the software engineers. This requires efficient communication considering different expertise and perspectives. Additionally, the two groups utilize language and communication tools in disparate ways. This, in turn, may lead to hidden misunderstandings in the requirement analysis phase and potentially result in implementation problems later on, that is difficult and costly to correct. In this paper, we demonstrate the above mentioned challenge via a use case from the tunneling domain. In particular, during the requirement analysis phase for a software capable of handling the data model of the subsoil. The domain experts in the field can best express the complexity of their domain by describing its artifacts, which in most cases are incomprehensible to the software engineers. We outline a method that interleaves requirement analysis and software modeling to enable an iterative increase of the accuracy and completeness of the information extracted from those artifacts and integrated into a flexible software model, which can produce testable software code automatically. Furthermore, we present a prototypical implementation of our method and a preliminary evaluation of the approach.",Paskaleva Galina; Mazak-Huemer Alexandra; Villeneuve Marlène; Waldhart Johannes,review:Decision=excluded; review:Time=2025-09-05 15:16,10.36680/j.itcon.2023.019,,2023,0,2025-09-05 15:16,
COMPARISON OF DISCRETIZATION METHODS FOR PREPROCESSING DATA FOR PYRAMIDAL GROWING NETWORK CLASSIFICATION METHOD,"This paper presents a comparison of four representative discretization methods from different classes to be used with so called PGN-classifier which deals with categorical data. We examine which of them supplies more convenient discretization for PGN Classification Method. The experiments are provided on the base of UCI repository data sets. The comparison tests were provided using an experimental classification machine learning system ""PaGaNe"", which realizes Pyramidal Growing Network (PGN) Classification Algorithm. It is found that in general, PGN-classifier trained on data preprocessed by Chi-merge achieve lower classification error than those trained on data preprocessed by the other discretization methods. The comparison of PGN-classifier, trained with Chi-merge-discretizator with other classifiers (realized in WEKA system) shows good results in favor of PGNclassifier.",Markov Krassimir; Ivanova Krassimira; Vanhoof Koen; Velychko Vitalii; Mitov Ilia; Stanchev Peter,review:Decision=excluded; review:Time=2025-09-05 15:17,,,,0,2025-09-05 15:17,
Evaluating SPARQL-based model checking: Potentials and limitations,"Model checking is an important task in the BIM collaboration process to prevent expensive planning errors. The submodels of the individual disciplines are transferred into a coordination model. Part of the transfer is a conversion into an exchange format. The exchange format allows the import into the model checking application. In the model checking application routines are performed to check the model against collisions and building regulation violations. During the transfer into the exchange format, information may get lost, especially with parameters that are not yet part of the exchange format supported by the authoring software. In recent years, ontologies have been investigated as a feasible approach to combine the submodels, since they model data in a flexible manner. Hence in the conversion process to an application-specific ontology, the data structure of the submodels can widely persist, which could lead to smaller information loss in comparison to converting the data into a standardized exchange format. The evaluation of the geometric properties of the building is indispensable for detecting and analyzing collisions. The basis for the connection of the different sub models could be the BOT (Building Topology Ontology), which defines the topological structure of a building and can be used to represent further building information by linking it with other ontologies. The relevant geometric relationships for the collision model checks have to be derived with a geometry kernel. For the research in this paper pythonOCC, a wrapper for the geometry kernel Open CASCADE is used with the Semantic Web's own query language SPARQL, queries can be formulated to analyze the collision relationships in combination with other semantic information. These queries can be used to verify model correctness. By connecting the information from different domains, more sophisticated tests are possible than in an exchange format dependent model checking application. The goal is to integrate the developed functionalities into a project platform. This platform is based on an extensive project description in an ontology-based data model and is connected to different authoring tools for the exchange of information.",Hoffmann A; Shi M; Wagner A; Thiele C-D; Huyeng T-J; Rüppel U; Sprenger W,review:Decision=included; review:Time=2025-09-05 15:17,,,2021,1,2025-09-05 15:17,
,,,,,,,,,
A survey of mathematical and informational foundations of the bigarm access method,"The BigArM is an access method for storing and accessing Big Data. It is under development. In this survey we present its mathematical and informational foundations as well as its requirements to realization characteristics. Firstly, we outline the needed basic mathematical concepts, the Names Sets, and hierarchies of named sets aimed to create a specialized model for organization of information bases called “Multi-Domain Information Model” (MDIM). The “Information Spaces” defined in the model are kind of strong hierarchies of enumerations (named sets). Further we remember the main features of hashing and types of hash tables as well as the idea of “Dynamic perfect hashing” and “Trie”, especially – the “Burst trie”. Hash tables and tries give very good starting point. The main problem is that they are designed as structures in the main memory which has limited size, especially in small desktop and laptop computers. To solve this problem, dynamic perfect hashing and burst tries will be realized as external memory structures in BigArM.",Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:16,,,2015,0,2025-09-05 15:16,
About NL-addressing,"В настоящей работе представлена идея естественно-языковой адресации. Это дополнительная возможность для представления онтологической информации в интеллектуальных системах. Естественно-языковая адресация имеет ряд преимуществ. На первом месте – это линейная алгоритмическая сложность, которая зависит от максимальной длины слов (max_L), а не от их количества. Во-вторых, это уменьшение объема занимаемой памяти – дополнительные индексы не используются. В-третьих, уменьшение времени обработки из-за полного отсутствия поиска – информация извлекается прямо по адресу. Необходимо отметить, что это универсальное представление информации одновременно доступной как для человека, так и для автоматизированных систем. Такой способ организации информации применим для ее хранения и использования в библиотеках онтологий, терминов, понятий, текстовых документов",Velychko Vitalii; IVANOVA Krassimira; MARKOV Krassimir,⛔ No DOI found; Естественно-языковая адресация; организация онтологических баз данных; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2012,0,2025-09-05 15:18,
Increased efficiency in virtual commissioning with automated model generation based on component libraries,"The use of Virtual Commissioning (VC) is becoming increasingly relevant for the engineering of automation systems. VC allows for validating concepts and design solutions in the early phases of product development, thereby improving communication among disciplines, reducing the costs for bug fixing and accelerating the overall engineering process, in particular the commissioning of plants and machines. However, as an additional software tool in engineering, the introduction of VC causes initial training and more importantly continuous extra modelling efforts. An approach to overcome this flaw is the automated generation of VC models based on existing engineering information from the system or domain level – ideally in tandem with the reuse of solution models from previous project. The latter can prove particularly beneficial in the context of modular building-block systems as often pursued in automation engineering.",Pyschny Nicolas; Rudat Ben; Permin Eike,review:Decision=excluded; review:Time=2025-09-05 15:16,10.1016/j.procir.2022.05.258,,2022,0,2025-09-05 15:16,
An Ontology-Driven Approach to Electronic Document Structure Design,"Over the course of history, humankind used documents as one of the ways of organization of the data. In the recent decades, electronic documentation became increasingly widespread. To make electronic documents exchange possible, standards regulating transmission protocols, representation formats, and rules for document building are necessary. For some protocols (HTTP, SOAP, etc.) and formats (EDI, XML, JSON, etc.), relatively fixed and generally accepted standards are available. As for the electronic document design, there is an abundance of approaches where a leader could hardly be established; all of them have their benefits and drawbacks. This study explores some of these approaches (UN/CEFACT CCTS, WCO DM, ISO 20022, and NIEM). These approaches have different features but from the conceptual perspective they are intended to describe sets of details of some real-world objects. The paper proposes to describe such objects using an ontology and then, based on this ontology, build conceptual structures of electronic documents that can be converted to platform-independent structures of electronic documents in accordance with one of the standards. The introduced approach allows harmonizing the standards under consideration.",Ignatov Dmitry I.; Khachay Mikhail Yu.; Labunets Valeri G.; Loukachevitch Natalia; Nikolenko Sergey I.; Panchenko Alexander; Savchenko Andrey V.; Vorontsov Konstantin; Nikiforov Denis A.; Korchagin Alexander B.; Sivakov Ruslan L.,review:Decision=included; review:Time=2025-09-05 15:17,,,2017,1,2025-09-05 15:17,
,,,,,,,,,
,,,,,,,,,
Using Domain-Specific Models to Facilitate Model-Based Systems-Engineering: Development Process Design Modeling with OPM and PROVE,"Model-based Systems Engineering (MBSE) approaches are a step forward in the evolution of computer-aided engineering, and yet, they often incorporate deﬁciencies that may jeopardize their practical utility and usability, as well as the validity of the resulting models. We demonstrate how a domain-speciﬁc modeling approach can relieve some hurdles in adopting MBSE, and how it can be used in tandem with a general-purpose modeling approach to augment and introduce rigor to models. Speciﬁcally, we demonstrate the consequences of theoretical issues that were previously identiﬁed in Object Process Methodology and suggest an approach to solve them. We use a generalized casestudy—derived from extensive process modeling in both academia and industry—to show that a domain-speciﬁc model can signiﬁcantly relax the user’s modeling effort. This demonstration is based on two quantitative metrics: the number of representational elements and available modeling tactics. We discuss the contribution of our approach to model quality, particularly with respect to its rigor and communicability.",Shaked Avi; Reich Yoram,Not Included; review:Decision=included; review:Time=2025-09-05 15:15,10.3390/app11041532,,2021,1,2025-09-05 15:15,
,,,,,,,,,
Class association rule mining using multidimensional numbered information spaces,"Data mining is of great importance in the overall process of knowledge discovery. In this dissertation we focused our attention in the part of discoveryoriented methods and especially classification algorithms. Class-Association Rules (CAR) algorithms have a special place within the family of classification algorithms. This type of classifiers offers a number of advantages: efficiency of the training regardless of the training set; easy handling with high dimensionality; very fast classification; high accuracy; classification model easily comprehensible for humans. The main classification workflow of CAR algorithms usually involves three phases: generating the rules, pruning, and recognition. The mining of association rules is a typical data mining task that works in an unsupervised manner. A major advantage of association rules is that they are theoretically capable to reveal all interesting relationships in a database. But for practical applications the number of mined rules is usually too large to be exploited entirely. Hence, a pruning phase is applied in order to build accurate and compact classifiers. The pruning can be applied during preprocessing, simultaneously to the association rules mining, or during post-processing. Different rule quality measures and rule ordering schemes can be applied in the process of rule selection. There are also different options which can be considered for the recognition phase – e.g. to use a simple rule or to use a set of rules with different types of ordering schemas. On the other hand, the process of creating classification models inevitably touches upon the use of appropriate access methods which facilitate access to different kinds of structures used in such algorithms. Our effort had been focused on the memory organization called Multidimensional numbered information spaces which allows to operate with contextfree multidimensional data structures. The program realization of such structures is named ArM 32. Multi-Domain Information Model (MDIM) and respectively Arm 32 are based on the process of replacing names by numbers which allows to use mathematical functions and addressing vectors for accessing the information. Our approach is to use such structures and operations in the implementation of one class association rule classifier in order to provide evidence on the vitality of the idea of using context-free multidimensional data structures and direct access as a powerful tool for knowledge discovery. We have proposed two classification algorithms – Pyramidal Growing Networks (PGN) and Multi-layer Pyramidal Growing Networks (MPGN). PGN creates association rules, optimized for maximal accuracy of produced rules. One of the main characteristics of PGN is that it is a parameter-free classifier. The association rule mining is executed from the longest rules to the shorter ones until no intersections between patterns in the classes are possible. In the pruning phase the contradictions and inconsistencies of more general rules are cleared, after that the pattern set is compacted excluding all more concrete rules within the classes. PGN is introduced as a useful tool for questioning the support-first principle used by many associative classifiers when mining for association rules. PGN reverses the common approach and focuses primarily on the confidence of the association rules and only in a later stage on the support of the rules. The main purpose is twofold: to provide a proof of concept for this new approach and to gather evidence on its potential. MPGN is based on multilayer structure. It involves possibility to escape combinatorial explosion using smart disposing of the information in the multilayer structures called ""pyramids"". These structures can be easily implemented using ArM-structures. These algorithms are implemented in the data mining environment PaGaNe, developed by the team from the Institute of Mathematics and Informatics – Bulgarian Academy of Sciences; Iliya Mitov and Krassimira Ivanova are the principal developers. PaGaNe incorporates different types of statistical analysis methods, discretization algorithms, association rule miner, as well as classification algorithms, which all are based on the use of multi-dimensional numbered information spaces. The Lenses dataset is used as a test example to illustrate the specifics of the proposed algorithms, the process of creating classification models as well as the process of recognition. We demonstrate that PGN produces the pattern set that is both minimal and complete for covering the learning set, which is an indicator for expectation that PGN will produce tight model and good accuracy results. In the case of MPGN we have demonstrated the process of creating main construction elements. We also have illustrated the functionality which allows to visualize how the pyramids are being created and how the queries are being recognized. We carried out experiments with 25 datasets from the UCI machine learning repository [Frank and Asuncion, 2010]. The experiments had been conducted using the data mining environment PaGaNe, the knowledge analysis system Weka, and LUCS-KDD Repository. A comparison between PGN, MPGN and some other CAR algorithms, as well as decision tree and decision rule classifiers which have similar behavior of creating the task model, had been done. One series of experiments aimed to study what accuracy had been obtained while preprocessing real data with different discretizators realized in PaGaNe. We found that in general PGN-classifier trained on data preprocessed by Chimerge with 95trained on data preprocessed by the other discretization methods. The main reason for this is that using Chi-square statistical measure as criterion for class dependency in adjacent intervals of a feature results in good separation between class labels. A second set of experiments studied the process of growing the learning sets and how this reflects on the classification model and the accuracy of PGN and MPGN; more specifically, we studied the critical point of the amount of the learning set in which classification model is relatively compact and the received accuracy stabilizes. Of course this critical point highly depends on the choice of dataset. A third set of experiments were focused on analyzing different exit points of MPGN. The received results showed that in a lot of cases the build constructs lead to excluding only one class as best competitor. Other cases usually fall into competition between classes, where different strategies for ordering the competitors can be applied. A very few cases fall into the way where MPGNalgorithm did not work and alternative choice is given. A fourth set of experiments aimed to analyze the dependencies of classifiers' behaviors when the noise rush in the dataset attributes; for this set we used the Monks1 dataset. The experiments demonstrated that noising in the dataset worsens considerably the accuracy of PGN which had been designed to perform well in clear datasets. However, experiments with other existing classifiers showed that they also were not been able to resist noising attacks. We made the comparison of overall accuracy between PGN, MPGN (with two recognition strategies – S1 and S2), CMAR, OneR, JRip, J48 and REPTree. The Friedman test showed statistical difference between tested classifiers. The post-hoc Nemenyi test showed that our PGN has best overall performance between examined classifiers and MPGN is competitive with CMAR, J48, JRip and REPTree. The experimental results are very positive and show that PGN is competitive with classification methods that build similar classification behavior. At the same time, it has an essential advantage over the other classifiers being parameter free. Furthermore, the empirical results showed that PGN is slightly more sensitive to noise than techniques such as C4.5 and RIPPER. However, its overall accuracy was still very good compared to these classifiers. In general, the results provide evidence that the confidence-first approach yields interesting opportunities for knowledge discovery.",MITOV Iliya,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2011,0,2025-09-05 15:17,
,,,,,,,,,
,,,,,,,,,
Knowledge integration via the fusion of the data models used in automotive production systems,"In this paper a novel information model that can be used in Manufacturing Execution Systems is presented. The model is based on the fusion of ISA95, AML and OPC UA. ISA95 is used to define, unify and describe the details of a product and production technology. It also enables communication with ERP systems. The AML standard allows information about the production facilities to be presented. The OPC UA address space represents different parts of an information model while the OPC communication protocol enables it to be linked to actual production systems. The proposed concept is illustrated using an actual example of a production line for electronic devices.",Cupek Rafal; Ziebinski Adam; Drewniak Marek; Fojcik Marcin,review:Decision=excluded; review:Time=2025-09-05 15:17,10.1080/17517575.2018.1489563,,2019,0,2025-09-05 15:17,
,,,,,,,,,
Knowledge integration via the fusion of the data models used in automotive production systems,"In this paper a novel information model that can be used in Manufacturing Execution Systems is presented. The model is based on the fusion of ISA95, AML and OPC UA. ISA95 is used to define, unify and describe the details of a product and production technology. It also enables communication with ERP systems. The AML standard allows information about the production facilities to be presented. The OPC UA address space represents different parts of an information model while the OPC communication protocol enables it to be linked to actual production systems. The proposed concept is illustrated using an actual example of a production line for electronic devices.",Cupek Rafal; Ziebinski Adam; Drewniak Marek; Fojcik Marcin,review:Decision=excluded; review:Time=2025-09-05 15:17,10.1080/17517575.2018.1489563,,2019,0,2025-09-05 15:17,
,,,,,,,,,
,,,,,,,,,
ArmSquare: An association rule miner based on multidimensional numbered information spaces,"In this article, we propose a simple approach for association rule mining, which uses the possibilities of the multidimensional numbered information spaces as a storage structures. The main focus in the realization of ArmSquare is using the advantages of such spaces, i.e., the possibility to build growing space hierarchies of information elements, the great power for building interconnections between information elements stored in the information base, and the possibility to change searching with direct addressing in well structured tasks. The tested types of implementations of realized tool show the vividness of proposed approach.",MITOV Iliya; IVANOVA Krassimira; DEPAIRE Benoit; VANHOOF Koen,⛔ No DOI found; Association Rule Mining; Market Basket Analysis; Multidimensional Numbered Information Spaces; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2011,0,2025-09-05 15:18,
Comparison of discretization methods for preprocessing data for pyramidal growing network classification method,"This paper presents a comparison of four representative discretization methods from different classes to be used with so called PGN-classifier which deals with categorical data. We examine which of them supplies more convenient discretization for PGN Classification Method. The experiments are provided on the base of UCI repository data sets. The comparison tests were provided using an experimental classification machine learning system ""PaGaNe"", which realizes Pyramidal Growing Network (PGN) Classification Algorithm. It is found that in general, PGN-classifier trained on data preprocessed by Chi-merge achieve lower classification error than those trained on data preprocessed by the other discretization methods. The comparison of PGN-classifier, trained with Chi-merge-discretizator with other classifiers (realized in WEKA system) shows good results in favor of PGNclassifier.",Markov Krassimir; Ivanova Krassimira; Vanhoof Koen; Velychko Vitalii; Mitov Ilia; Stanchev Peter,review:Decision=excluded; review:Time=2025-09-05 15:17,,,,0,2025-09-05 15:17,
,,,,,,,,,
Algorithm for quick numbering of large volumes of data,"An original algorithm for numbering large datasets by means of Natural Language Addressing (NLA) is presented in the paper. We use a counter to number different instances and store its current value in the container NL-addressed by the instance. If the instance is repeated, from this NL-address we receive its already assigned number. The algorithm is implemented in an experimental program RDFArM for storing large RDF-datasets. The provided experiments have shown that NL-access time for one instance (triple or quadruple) does not depend on number of already stored instances from the dataset. This is very important for storing Big Data.",Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2015,0,2025-09-05 15:18,
Decentralization in industry 4.0 supported by opc ua multi-domain information models: Case study and implementation challenges,"This paper introduces an OPC UA multi-domain information model aimed at supporting decentralization in industrial automation and includes a case study addressing the implementation challenges associated with integrating standards such as Device, PackML, ISA95, and AAS. The developed model enables interoperability and harmonization of information models across various domains, which is crucial for Industry 4.0 applications. The work highlights the paradox of decentralization, where independent modules require centralized control and data coordination, necessitating a unified information model. Key contributions include the proposal of methods to address technical challenges related to data type compatibility and inheritance of properties across domains. Additionally, the text emphasizes the importance of system integration for data acquisition from physical assets. A comparative analysis of implementation on PLCs, on-premises computing platforms, and cloud environments highlights tradeoffs in deployment depending on system complexity and response time requirements. The study results may assist in developing an information model for a decentralized unit in Industry 4.0.",Pribiš Rudolf; Beňo Lukáš; Pajpach Martin; Drahoš Peter; Kocák Ondrej,Interoperability; multi-domain information model; Data models; Computational modeling; decentralization; Fourth Industrial Revolution; Industry 4.0; OPC UA; Proposals; Servers; Standards; Synchronization; System integration; Time factors; review:Decision=included; review:Time=2025-09-05 15:16; review:Reason=Om een reden,10.1109/KI64036.2025.10916421,,2025,1,2025-09-05 15:16,Om een reden
,,,,,,,,,
Big data addressed in natural language,"Actions related to the cloud processing of large volumes of semi- or unstructured, streaming data (so-called “Big Data”) pose challenges that organizations must address. They are related to the location of the data in the cloud, its storage and management. Much of Big Data is collected from sources that are external to the business organization, such as social media, demographics, web data, events, news sources, and more. In this article, we discuss an access method to enable support for very large Big Data databases. It is based on the capabilities of Natural Language Addressing (NLA). An important advantage of NLA is the reduction of the amount of occupied memory due to the complete absence of additional indexes, absolute addresses, pointers and additional files, as well as the reduction of processing time due to the complete absence of searches - data is stored / retrieved to / from a direct address.",Karastanev Stefan; Markova Vera; Ivanova Krasimira,Data models; Big Data; big data databases; BigNLA access method; Memory management; natural language addressing; Natural languages; Organizations; Social networking (online); Task analysis; review:Decision=excluded; review:Time=2025-09-05 15:17,10.1109/AEIS61544.2023.00016,,2023,0,2025-09-05 15:17,
Automatic Metadata Generation and Digital Cultural Heritage,,Ivanova Krassimira; Mitov Iliya; Depaire Benoit; Vanhoof Koen; Blagoev Dimitar,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2012,0,2025-09-05 15:17,
,,,,,,,,,
ArmSquare: An association rule miner based on multidimensional numbered information spaces,"In this article, we propose a simple approach for association rule mining, which uses the possibilities of the multidimensional numbered information spaces as a storage structures. The main focus in the realization of ArmSquare is using the advantages of such spaces, i.e., the possibility to build growing space hierarchies of information elements, the great power for building interconnections between information elements stored in the information base, and the possibility to change searching with direct addressing in well structured tasks. The tested types of implementations of realized tool show the vividness of proposed approach.",MITOV Iliya; IVANOVA Krassimira; DEPAIRE Benoit; VANHOOF Koen,Association Rule Mining; Market Basket Analysis; Multidimensional Numbered Information Spaces; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2011,0,2025-09-05 15:18,
,,,,,,,,,
Managerial and Ontological Issues in the Development of Enterprise Architecture: Experiences from a Case Study,"With increasing numbers and complexity of information models, it is becoming more and more important to have supporting modeling frameworks. Previous literature within this field has been descriptive, conceptual, and normative with few empirical studies addressing managerial and cognitive challenges during the development of large-scale models. This article addresses modeling from three perspectives: Enterprise Architecture (EA), Ontology, and Management Theory. The article presents the experiences from a large corporation effort to develop a modeling framework. The focus is on managerial and cognitive issues related to the development and use of the modeling framework. The conclusion is that large-scale modeling activities are complex involving several cognitive issues, such as managing different interpretations of terms due to different languages, modeling frameworks, cultures, and information systems.",Hedman Jonas; Schonström Mikael,⛔ No DOI found; review:Decision=included; review:Time=2025-09-05 15:12,,,2013,1,2025-09-05 15:12,
ADVANCE OF THE ACCESS METHODS,The goal of this paper is to outline the advance of the access methods in the last ten years as well as to make review of all available in the accessible bibliography methods.,Markov Krassimir; Ivanova Krassimira; Karastanev Stefan; Mitov Ilia,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2008,0,2025-09-05 15:17,
An ontology-driven approach to electronic document structure design,"Over the course of history, humankind used documents as one of the ways of organization of the data. In the recent decades, electronic documentation became increasingly widespread. To make electronic documents exchange possible, standards regulating transmission protocols, representation formats, and rules for document building are necessary. For some protocols (HTTP, SOAP, etc.) and formats (EDI, XML, JSON, etc.), relatively fixed and generally accepted standards are available. As for the electronic document design, there is an abundance of approaches where a leader could hardly be established; all of them have their benefits and drawbacks. This study explores some of these approaches (UN/CEFACT CCTS, WCO DM, ISO 20022, and NIEM). These approaches have different features but from the conceptual perspective they are intended to describe sets of details of some real-world objects. The paper proposes to describe such objects using an ontology and then, based on this ontology, build conceptual structures of electronic documents that can be converted to platform-independent structures of electronic documents in accordance with one of the standards. The introduced approach allows harmonizing the standards under consideration.",Ignatov Dmitry I.; Khachay Mikhail Yu.; Labunets Valeri G.; Loukachevitch Natalia; Nikolenko Sergey I.; Panchenko Alexander; Savchenko Andrey V.; Vorontsov Konstantin; Nikiforov Denis A.; Korchagin Alexander B.; Sivakov Ruslan L.,review:Decision=included; review:Time=2025-09-05 15:17,10.1007/978-3-319-52920-2_1,,2017,1,2025-09-05 15:17,
,,,,,,,,,
,,,,,,,,,
Automatic metadata generation and digital cultural heritage,,Ivanova Krassimira; Mitov Iliya; Depaire Benoit; Vanhoof Koen; Blagoev Dimitar,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2012,0,2025-09-05 15:17,
,,,,,,,,,
Multi-layer knowledge representation,"An approach for knowledge representation based on post-relation type of information bases is outlined in the paper. Explanation starts with remembering the idea of Natural Language Addressing. After that, the idea of Multi-layer Knowledge Representation by Means of Natural Language Addressing is presented.",Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2014,0,2025-09-05 15:18,
An approach to multi-domain data model development based on the model-driven architecture and ontologies,"To date, there are many diverse data representation technologies (EDIFACT, XML, JSON, CSV, relational model, NoSQL). Transition to new technologies or the integration of information systems based on different technological stacks is a complex and expensive process. Platform-independent models take an important role in this process. The structure of such a model is described in this article. However, given the data model has been created at the junction of different domains, it may be not enough. In such case, a one more step of abstraction and a movement to the computation-independent model is required. The authors propose to create it in an ontological form.",Nikiforov Denis A; Lisikh Igor G; Sivakov Ruslan L,⛔ No DOI found; review:Decision=included; review:Time=2025-09-05 15:16,,,2015,1,2025-09-05 15:16,
Example of multi-layer knowledge representation by means of natural language addressing,An approach for knowledge representation based on post-relation type of information bases is outlined in the paper. The idea of Natural Language Addressing and based on it idea of Multi-layer Knowledge Representation are presented.,Ivanova Krassimira,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:18,,,2014,0,2025-09-05 15:18,
,,,,,,,,,
Data independence in the multi-dimensional numbered information spaces,The concept of data independence designates the techniques that allow data to be changed without affecting the applications that process it. The different structures of the information bases require corresponded tools for supporting data independence. A kind of information bases (the Multi-dimensional Numbered Information Spaces) are pointed in the paper. The data independence in such information bases is discussed.,Markov Krassimir,⛔ No DOI found; review:Decision=excluded; review:Time=2025-09-05 15:17,,,2008,0,2025-09-05 15:17,
,,,,,,,,,
